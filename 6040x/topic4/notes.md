# Representing Numbers
- Numbers are represented by a string of digits, which represent a mathematical value
- Each numeric place: is the base to the ith power
- Computers use binary
  - bit: BInary digiT
- Fractional values
  - 0.1 base 10 is an infinite decimal in base 2
  - Many seemingly finite decimals are infinite in binary, so note that memory limitations lead to approximations
- Floating points
  - Fixed digits in the string
  - Decimal point can go anywhere among the digits (it floats)
  - Any real number can be written in scientific notation, then recorded as a tuple:
    - (sign, significand, exponent)
  - Usually fix the size of the significand and exponent digits
- Rounding errors
  - Have to store numbers in a finite format, requires rounding at some point
- Single-precision
  - 'float' type in C, Java, etc. 32 bits total
    - 1 sign bit
    - 24 bit significand
    - 8 bit exponent
  - Smallest possible positive value:
    - 2 ^ -126
  - Smalles possible positive value > 1
    - 1.00000001 ^ 1
    - the rounding error here is epsilon
      - 'machine epsilon' - depends on # of bits in the significand
      - here, 2^-23
- Double-precision
  - 'double' in primitive languages
  - default type in Python
  - 64 bits
    - 1 sign bit
    - 53 bit signficand
    - 11 bit exponent
  - Much more precise, but still has roundoff error
- Error analysis
  - f(x) is the exact function to compute
  - alg(x) approximates f(x)
  - what is difference b/w outputs of them?
    - want to minimize the difference - the 'forward error'
    - if it's 'small enough', is 'forward stable'
  - alg(x) = f(x + \delta * x)
    - 'backwards stability' analysis
    - \delta * x is the backward error
    - Want to show that the backward error is small
      - Show that it's 'backward stable'
  - Backward error is a bit more appropriate for computer programs - want similar noisy inputs to yield similar outputs
  - If you show the backward error is small, then that implies the forward error is also small (Taylor theorem)
  - float(a+b) = (a + b)(1+\delta)
    - \delta is the relative error, rarely know its value
    - But do know its worst case scenario, \epsilon
  
