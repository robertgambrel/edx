---
title: "Gambrel 6501x HW 2"
author: "Robert Gambrel"
date: "May 23, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r wd}
rm(list = ls())

if (Sys.info()[[1]] == "Windows") {
  setwd("E:/GoogleDrive/edx/6501x/week2")
} else if (Sys.info()[[1]] == "Darwin") {
  setwd("/Users/healthpolicyanalyst/Documents/Box Sync/Personal/edx/6501x/week2")

}

pacman::p_load(dplyr, tidyr, magrittr, readr, purrr, stats, outliers, lubridate, ggplot2)
set.seed(42)
``` 
# Q1

Political scientists frequently ask respondents a range of questions to gauge respondents' attitudes towards social, economic, and security issues. These questions can then be used to categorize individuals into political clusters (libertarian, populist, progressive, conservative, etc.) that are more fine-grained than simple party affiliation. Predictors useful in categorizing voters could include:
1. Support for government sponsored universal health insurance
2. Support for flat vs. progressive tax systems
3. Attitudes towards social issues and government's role in proscribing behaviors
4. Attitudes towards the criminal justice system (role of police, drug reform, prison sentencing guidelines)

# Q2

```{r}
data(iris)
n_distinct(iris$Species)
iris$Species %>% unique()

kplot <- function(i) {
  means <- kmeans(iris[, 1:4], centers = i, nstart = 50)
  plot(iris[, 1:4], col = means$cluster)
  }
  
walk(1:5, kplot)
```
Reviewing the output, it appears that a three cluster approach is the best approach. It does a reasonable job separating the data without adding excessive sub-clusters that border each other too much.

```{r}
means <- kmeans(iris[, 1:4], centers = 3, nstart = 50)

print(means$centers)
```
After applying a k=3 model, I find that all 4 predictor variables contribute to clustering. The centers of each of the 3 clusters are displayed above.

```{r}
fitted.means <- fitted(means, method = "classes")

fitted_df <- as.data.frame(fitted.means)
fitted_df$class <- NA_character_
fitted_df <- fitted_df %>%
    mutate(
        class = ifelse(fitted.means == 3, 'setosa', 
                          ifelse(fitted.means == 2, 'virginica', 'versicolor')))
                          
sum(fitted_df$class == iris$Species) / nrow(fitted_df)
```
Comparing predicted clusters versus actual flower type yields a predictive accuracy of 89.33%.

# Q3
```{r}
crime <- read_tsv("http://www.statsci.org/data/general/uscrime.txt")

summary(crime$Crime)

crime %>%
  filter(Crime >= quantile(Crime, .95))

crime %>%
  filter(Crime <= quantile(Crime, .05))

outliers::grubbs.test(crime$Crime, 10)
outliers::grubbs.test(crime$Crime, 10, opposite = T)
outliers::grubbs.test(crime$Crime, 11)
```

The highest Crime ratio is 1993, but the second highest is 1969. The top value is therefore not more extreme than the next. Similarly for the lowest value (342), the second lowest is 373. Given the small sample size, these do not appear to be abnormally far from the median. The Grubbs test supports this conclusion - no outlier test reaches statistical significance.


# Q4

A CUSUM model might be useful in determining at what point in the day traffic has transitioned from normal flow to rush-hour backups. By knowing both regular rush-hour times as well as identifying days that typically start earlier or later, emergency responders can make necessary continency plans about travel times.

Such a model could use location data recorded by navigation devices and apps to determine mean travel times between select business districts and suburban areas. As commuters began their daily travel, the new travel times could be compared against the mean to determine whether travel had significantly slowed.

Determining the threshold value will be more important than the C damper. While some drivers might successfully weave across lanes to increase average speed, the overall 'flow of traffic' should be relatively consistent across cars experiencing the same backup. Therefore the C value could be relatively small - perhaps only a few minutes, to account for random error. The T threshold value would be more difficult, as inconvenience due to rush hour delays are subjective. I would probably shoot for something like a 50% increase in travel time as my rush hour threshold. So, if the usual commute is 40 minutes, but travel times are currently at one hour, I would want the model to classify current conditions as rush hour traffic.

# Q5
I first downloaded the data, cleaned it, and saved it as a csv file.
```{r}
# Question 5

temps <- read_tsv('temps.txt')

temps_long <- temps %>%
  gather(year, high_temp, 2:ncol(temps))

temps_long %<>%
	group_by(year) %>%
	mutate(
		rn = 1:n()
	)


ggplot(data = temps_long) +
	geom_line(aes(x = rn, y = high_temp, col = as.factor(year), group = year)) +
	geom_smooth(aes(x = rn, y = high_temp), method = 'loess') +
  geom_hline(aes(yintercept = mean(temps_long$high_temp, na.rm = T))) +
	scale_color_discrete(name = "Year")
```
Looking at the data, there is clearly a downward trend starting towards the end of summer. To calculate a CUSUM approach, I recreate the model shown in the lecture:

```{r}
  
T = 10
C = 0

cusum <- function(T, C) {
cusum_data <- temps_long %>%
  # mu by year
  group_by(year) %>%
    mutate(
    mu = mean(high_temp, na.rm = T)
    ) %>%
  ungroup() %>%
  # CUSUM Model
  arrange(year, rn) %>%
  mutate(
    mu_minus_temp = mu - high_temp,
    mu_minus_temp_minus_c = mu - high_temp - C,
    S = 0
  )  %>%
  # group by year so no cross-year lags being checked
  group_by(year) %>%
  mutate(
    S_old = ifelse(lag(S) + mu_minus_temp_minus_c > 0 & !is.na(lag(S)),
                   lag(S) + mu_minus_temp_minus_c, 0),
    S = S_old,
    tester = lag(high_temp)
  ) %>%
  ungroup()
  
first_occur <- cusum_data %>% 
  # only days above the threshold
  filter(
    S >= T
  ) %>%
  # keep earliest observation per year
  group_by(year) %>%
  slice(1)
  
print(paste0('T = ', T, '; C = ', C, '; Mean Day = ', round(mean(first_occur$rn), 3),'; s.d. Day = ', 
	round(sd(first_occur$rn), 3)))
#first_occur[, c('Year', 'Month', 'Day', 'S')]
}

T_list <- c(5, 10, 15, 20)
C_list <- c(0, 3, 6, 9, 12)

cross2(T_list, C_list) %>%
	walk(lift(cusum))

cross2(c(5), 0:20) %>%
	walk(lift(cusum))
```
