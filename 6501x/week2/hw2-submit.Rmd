---
title: "Gambrel 6501x HW 2"
author: "Robert Gambrel"
date: "May 23, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r wd}
rm(list = ls())

if (Sys.info()[[1]] == "Windows") {
  setwd("E:/GoogleDrive/edx/6501x/week2")
} else if (Sys.info()[[1]] == "Darwin") {
  setwd("/Users/healthpolicyanalyst/Documents/Box Sync/Personal/edx/6501x/week2")

}

pacman::p_load(dplyr, tidyr, magrittr, readr, purrr, stats, outliers, lubridate, ggplot2)
set.seed(42)
``` 
# Q1

Political scientists frequently ask respondents a range of questions to gauge respondents' attitudes towards social, economic, and security issues. These questions can then be used to categorize individuals into political clusters (libertarian, populist, progressive, conservative, etc.) that are more fine-grained than simple party affiliation. Predictors useful in categorizing voters could include:
1. Support for government sponsored universal health insurance
2. Support for flat vs. progressive tax systems
3. Attitudes towards social issues and government's role in proscribing behaviors
4. Attitudes towards the criminal justice system (role of police, drug reform, prison sentencing guidelines)

# Q2

```{r}
data(iris)
n_distinct(iris$Species)
iris$Species %>% unique()

kplot <- function(i) {
  means <- kmeans(iris[, 1:4], centers = i, nstart = 50)
  plot(iris[, 1:4], col = means$cluster)
  }
  
walk(1:5, kplot)
```
Reviewing the output, it appears that a three cluster approach is the best approach. It does a reasonable job separating the data without adding excessive sub-clusters that border each other too much. The red group is consistently in its own cluster, and the green and black groups are contiguous but pretty consistently non-overlapping.

```{r}
means <- kmeans(iris[, 1:4], centers = 3, nstart = 50)

print(means$centers)
```
After applying a k=3 model, I find that all 4 predictor variables contribute to clustering. The centers of each of the 3 clusters are displayed above.

```{r}
fitted.means <- fitted(means, method = "classes")

fitted_df <- as.data.frame(fitted.means)
fitted_df$class <- NA_character_
fitted_df <- fitted_df %>%
    mutate(
        class = ifelse(fitted.means == 3, 'setosa', 
                          ifelse(fitted.means == 2, 'virginica', 'versicolor')))
                          
sum(fitted_df$class == iris$Species) / nrow(fitted_df)
```
Comparing predicted clusters versus actual flower type yields a predictive accuracy of 89.33%.

# Q3
```{r}
crime <- read_tsv("http://www.statsci.org/data/general/uscrime.txt")

summary(crime$Crime)

crime %>%
  filter(Crime >= quantile(Crime, .95))

crime %>%
  filter(Crime <= quantile(Crime, .05))

outliers::grubbs.test(crime$Crime, 10)
outliers::grubbs.test(crime$Crime, 10, opposite = T)
outliers::grubbs.test(crime$Crime, 11)
```

The highest Crime ratio is 1993, but the second highest is 1969. The top value is therefore not more extreme than the next. Similarly for the lowest value (342), the second lowest is 373. Given the small sample size, these do not appear to be abnormally far from the median. The Grubbs test supports this conclusion - no outlier test reaches statistical significance.


# Q4

A CUSUM model might be useful in determining at what point in the day traffic has transitioned from normal flow to rush-hour backups. By knowing both regular rush-hour times as well as identifying days that typically start earlier or later, emergency responders can make necessary continency plans about travel times.

Such a model could use location data recorded by navigation devices and apps to determine mean travel times between select business districts and suburban areas. As commuters began their daily travel, the new travel times could be compared against the mean to determine whether travel had significantly slowed.

Determining the threshold value will be more important than the C damper. While some drivers might successfully weave across lanes to increase average speed, the overall 'flow of traffic' should be relatively consistent across cars experiencing the same backup. Therefore the C value could be relatively small - perhaps only a few minutes, to account for random error. The T threshold value would be more difficult, as inconvenience due to rush hour delays are subjective. I would probably shoot for something like a 50% increase in travel time as my rush hour threshold. So, if the usual commute is 40 minutes, but travel times are currently at one hour, I would want the model to classify current conditions as rush hour traffic.

# Q5
I first downloaded the data, cleaned it, and saved it as a csv file.
```{r}
# Question 5

temps <- read_tsv('temps.txt')

temps_long <- temps %>%
  gather(year, high_temp, 2:ncol(temps)) %>%
  mutate(year = as.numeric(year))

temps_long %<>%
	group_by(year) %>%
	mutate(
		rn = 1:n()
	)


ggplot(data = temps_long) +
	geom_line(aes(x = rn, y = high_temp, col = as.factor(year), group = year)) +
	geom_smooth(aes(x = rn, y = high_temp), method = 'loess') +
  geom_hline(aes(yintercept = mean(temps_long$high_temp, na.rm = T))) +
	scale_color_discrete(name = "Year")

mean(temps_long[temps_long$rn <= 30,]$high_temp, na.rm = T)
mean(temps_long[temps_long$rn >= 90,]$high_temp, na.rm = T)

```
Looking at the data, there is clearly a downward trend starting around day 75, or the middle of August. Mean temperature in July is 88.8 degrees, while in October it is 73.7 degrees. To calculate a CUSUM approach, I recreate the model shown in the lecture. I calculate mu independently for each year.

```{r}
  

cusum <- function(T, C, print_result = F) {
  cusum_data <- temps_long %>%
    # mu by year
    group_by(year) %>%
      mutate(
      mu = mean(high_temp, na.rm = T)
      ) %>%
    ungroup() %>%
    # CUSUM Model
    arrange(year, rn) %>%
    mutate(
      mu_minus_temp = mu - high_temp,
      mu_minus_temp_minus_c = mu - high_temp - C,
      S = 0
    )  %>%
    # group by year so no cross-year lags being checked
    group_by(year) %>%
    mutate(
      S_old = ifelse(lag(S) + mu_minus_temp_minus_c > 0 & !is.na(lag(S)),
                     lag(S) + mu_minus_temp_minus_c, 0),
      S = S_old,
      tester = lag(high_temp)
    ) %>%
    ungroup()
    
  first_occur <- cusum_data %>% 
    # only days above the threshold
    filter(
      S >= T
    ) %>%
    # keep earliest observation per year
    group_by(year) %>%
    slice(1) %>%
    ungroup()
    
  print(paste0('T = ', T, '; C = ', C, '; Mean Day = ', round(mean(first_occur$rn), 3),'; s.d. Day = ', 
  	round(sd(first_occur$rn), 3)))
  
  if(print_result) {
    print(first_occur)
    return(first_occur)
  }
#first_occur[, c('Year', 'Month', 'Day', 'S')]
}

T_list <- c(5, 10, 15, 20)
C_list <- c(0, 3, 6, 9, 12)

cross2(T_list, C_list) %>%
	walk(lift(cusum))

```
While T = 15, C = 9 seems to have the most consistent results across the model, 15 degrees seems an excessive threshold to require temperatures to drop. I will therefore focus more on a threshold of 10:

```{r}
cross2(10, 0:20) %>%
	walk(lift(cusum))
```
By adding a higher dampener, the results become tighter around the chosen day the threshold is met. However, this loses sensitivity. Out of these, I would choose T = 10, C = 9. It represents a substantial decrease in s.d. of the threshold day across years when compared with lower C values and provides a reasonable estimate of when summer heat ends - early October. 

# 5.2
```{r}
threshold_dates <- cusum(10, 9, print_result = T)
ggplot(data = threshold_dates) +
  geom_line(aes(x = year, y = mu))
```
It appears that mean temperatures may be on a slight upward trend, but there is so much year-to-year fluctuation that it is hard to know for certain. 
```{r}
cusum_yearly <- function(T, C, print_result = F) {
  cusum_data <- threshold_dates %>%
    select(year, mu) %>%
    mutate(
      global_mu = mean(mu)
    ) %>%
    arrange(year) %>%
    mutate(
      temp_minus_mu = mu - global_mu,
      temp_minus_mu_minus_C = mu - global_mu - C,
      S = 0
    )  %>%
    mutate(
      S_old = ifelse(lag(S) + temp_minus_mu_minus_C > 0 & !is.na(lag(S)),
                     lag(S) + temp_minus_mu_minus_C, 0),
      S = S_old
      ) %>%
    ungroup()
    
  first_occur <- cusum_data %>% 
    # only days above the threshold
    filter(
      S >= T
    ) %>%
    slice(1)
    
  print(paste0('T = ', T, '; C = ', C, '; Threshold Year Met = ', round(first_occur$year)))
  
  if(print_result) {
    print(first_occur)
    return(first_occur)
  }
#first_occur[, c('Year', 'Month', 'Day', 'S')]
}

T_list <- c(1, 2, 3, 4)
C_list <- c(1, 2, 3, 4)

cross2(T_list, C_list) %>%
	walk(lift(cusum_yearly))
```
The results would lead me to conclude that temperatures have not increased in our observation period. A few successful reaches of the threshold are achieved, but these are only at low T values with small C values as well. For the majority of the models, the threshold is not met. Since each model has a random chance of falsely exceeding a threshold earlier than its true value would suggest, I would not say the temperatures have increased overall.
