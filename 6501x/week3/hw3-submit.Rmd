---
title: "Homework 3"
author: "Robert Gambrel"
date: "May 31, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Q1

I've been trying to lose a few pounds this summer. A lot of guidance online suggests checking weight daily but looking only at weekly averages, since daily fluctuations can be drastic and not reflective of actual fat loss. How much water/food was ingested recently, how much salt, how recently you used the bathroom, and other factors give each weigh-in a lot of randomness around the 'true' value. I would therefore use a low alpha - this would emphasize the long-term trend instead of the day-to-day fluctuations.

# Q2

```{r}
pacman::p_load(dplyr, readr, purrr, outliers, ggplot2, lubridate, tidyr, stats, smooth, tibble, magrittr)

# temps <- read_tsv('T:\\Clients\\WIP\\EJGH\\Data Science Team\\RG\\trainings\\temps-new.txt')
temps <- read_tsv('temps.txt')

temps_sorted <- temps %>%
	gather(year, high_temp, 2:21) %>%
	group_by(year) %>%
	mutate(rn = 1:n()) %>%
	ungroup() %>%	
	arrange(year, rn) %>%
	select(high_temp)

series <- ts(temps_sorted$high_temp, frequency = 123)

m <- HoltWinters(x = series, seasonal = 'mult', beta = F)
plot(m)
plot(fitted(m))
fitted <- m$fitted
seasons <- fitted[, 3]

seasons <- as.data.frame(seasons)
seasons %<>%
	mutate(
		rn = 1:n()
		) %>%
	mutate(
		year = ceiling(rn / 123)
	) %>%
	group_by(year) %>%
	mutate(day = 1:n()) %>%
	ungroup()

seasonals <- seasons %>%
	group_by(year) %>%
	summarize(
		mean_mult = mean(x, na.rm = T)
	)

firsts <- seasons %>%
	filter(x <= 0.9) %>%
	# keep first occurence of seasonal multiplier <= .95 each year 
	group_by(year) %>%
	slice(1)

seasonals
firsts

#m <- es(series, model = "AAM")
#plot(m)
#plot(fitted(m))
#m$model
#m$formula
#m$initial
#m$initialSeason
#fit <- as.tibble(m$fitted)
#fit$year <- 0

#series_short <- ts(temps_sorted$high_temp[1:123])
#m_short <- HoltWinters(x = series_short, beta = F, gamma = F)
```

```{r}
# fit %<>%
#   mutate(
#     n = 1:n()
#   )
# 
# for (i in 1:20) {
#   fit %<>%
#     mutate(
#       year = ifelse((i - 1)*123 < n & n <= i*123, i, year)
#     )
# }
# 
# fit %<>%
#   group_by(year) %>%
#   mutate(
#     rn = 1:n()
#   )
# 
# ggplot(data = fit[fit$year %in% c(1, 5, 10, 15, 20),]) +
#   geom_line(aes(x = rn, y = `Series 1`, col = as.factor(year), group = as.factor(year)))
#   
```

I fit a Holt Winters model to the data, using seasonal multipliers (period = 123 days). Since these values can update over time, I check two factors: first, whether the average yearly seasonal mulitplier is chaning year-to-year. Second, whether the seasonal multiplier falls below a threshold (0.9) earlier from year to year.

In both cases, the results don't suggest that summer is ending earlier.

# Q3

We recently had to buy a car and decided to purchase a used SUV. We consulted several resources to find a fair price, and all of them ask about several factors that determine a vehicle's dollar price. Things like age, mileage, and condition matter for pretty much every car. In addition, certian models have different available options (leather interior, bluetooth audio, backup cameras) that are binary options that can also add to the value of the car. 

# Q4
```{r}


crime <- read_tsv("http://www.statsci.org/data/general/uscrime.txt")


model <- lm(Crime ~ M + So + Ed + Po1 + Po2 + LF + M.F + Pop +
			  NW + U1 + U2 + Wealth + Ineq + Prob + Time, 
			  data = crime)
summary(model)

model2 <- lm(Crime ~ M + Ed + Po1 + Po2 +
			  U1 + U2 + Ineq + Prob, 
			  data = crime)
summary(model2)

AIC(model)
AIC(model2)

BIC(model)
BIC(model2)

new_data <- data.frame(M = 14.0, Ed = 10.0, Po1 = 12.0, Po2 = 15.5,
			     U1 = 0.120, U2 = 3.6, Ineq = 20.1, Prob = 0.04)

predict(model2, new_data)
```
I originally fit a model with every variable included. However, many did not relate to the outcome variable. I dropped all those that were not significant, except I kept variables that were part of a cluster of variables that had at least one significant variable (for example, I kept U1 since U2 was significant).

Removing these unnecessary variables improves the model's penalized fits: Adjusted R-squared increases from 0.70 to 0.73. AIC and BIC both drop by substantial amounts. 

Using the important variables from the hypothetical case, I predict that crime rates would be 820, which is just below the median state in the current data.
