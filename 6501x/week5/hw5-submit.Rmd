---
title: "Homework 5"
author: "Robert Gambrel"
date: "June 20, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (Sys.info()[[1]] == "Windows") {
  setwd("E:/GoogleDrive/edx/6501x/week2")
} else if (Sys.info()[[1]] == "Darwin") {
  setwd("/Users/healthpolicyanalyst/Documents/Box Sync/Personal/edx/6501x/week2")
} else if (Sys.info()[[1]] == "Linux") {
  setwd("~/Documents/edx/6501x/week5")
}

```

```{r}

pacman::p_load(dplyr, tidyr, magrittr, readr, purrr, stats, outliers, 
               lubridate, ggplot2, glmnet, FrF2, tibble, broom)
set.seed(42)
```

# Q1

## Stepwise
```{r}
crime <- read_tsv('uscrime.txt')
crime <- as_tibble(scale(crime))


# Start with just the response variable
expanding_dataset <- crime %>% select(Crime)

# pool of predictor variables to check
crime_predictor_pool <- crime[, 1:15]

# run until no viable predictors are left
while (T) {
  # initialize empty tracking table
  tracking_table <- data.frame()
  # check each variable in the pool
  # on the first loop, will be the only regressor. In later steps, it will be added
  # to previously selected variables
  for (i in 1:ncol(crime_predictor_pool)) {
    crime_model_data <- cbind(expanding_dataset, crime_predictor_pool[, i])
  
    model <- lm(Crime ~ ., data = crime_model_data)
    # Check the variable's p value. If it's low enough, add it to the tracking table
    current_p <- coef(summary(model))[length(coef(summary(model)))]
    
    if (current_p < 0.15) {
      tracking_table <- rbind(tracking_table, data.frame(i, current_p))
    }
  }
  
  # If there were any variables that had p < 0.15, add the BEST one to the 
  # list of variables to include
  if (nrow(tracking_table) >= 1) {
    best_column = tracking_table$i[tracking_table$current_p == min(tracking_table$current_p)]
    # add the best variable
    expanding_dataset <- cbind(expanding_dataset, crime_predictor_pool[, best_column])
    # drop it from the list of variables to check next round
    crime_predictor_pool <- crime_predictor_pool[, -best_column]
    # if no variables were good enough, then exit the loop
  } else break
  
  # Check whether to drop any vars, drop if they're too big now
  # they're not added back to predictor pool
  latest_model <- tidy(summary(lm(Crime ~ ., data = expanding_dataset)))
  
  drop_vars <- latest_model$term[latest_model$p.value > 0.15]
  
  expanding_dataset <- expanding_dataset[, !names(expanding_dataset) %in% drop_vars]
}

almost_model <- lm(Crime ~ ., data = expanding_dataset)
summary(almost_model)

```
After iteratively adding the most significant variable at each step (until no more with p < 0.15 were left to be added) and removing any that dropped below the threshold at each step, I check the final model. All of the variables in it are significant at p < 0.05, so I don't need to remove any.

## LASSO

```{r}
model <- glmnet(x = as.matrix(crime[, 1:15]), y = as.matrix(crime[, 16]), family = 'gaussian', alpha = 1)
plot(model, label = T)

```
Observing the coefficients chosen under different lambda (tau) restrictions, variable 4, or Po1, is always chosen as important. Similarly, variable 13, or Inequality, frequently has a fair amount of weight applied as well. When the restriction is very weak, most of the variables are included in the model. However, this makes the model susceptible to over-fitting and weaker predictive power, so cross-validation can be used to determine the most effective L1 value.

```{r}
cv.model <- cv.glmnet(x = as.matrix(crime[, 1:15]), 
                      y = as.matrix(crime[, 16]), 
                      family = 'gaussian', alpha = 1)
plot(cv.model)
coef(cv.model, s = 'lambda.min')
coef(cv.model, s = 'lambda.1se')
```
The first vertical line indicates the point at which mean cross-validated error is at a minimum. The second vertical line is the simplest model that is within one standard error of the minimum. The latter model shows that Po1, M.F, M, and Prob are all key variables in predicting crime rates. If we are OK with adding more variables and risking an overfitting problem, then we would bring in everything except Po2, LF, and Time.

## Elastic Net
To determine both optimal alpha and lambda values, I'll use cross validation for a range of values.
```{r}
folds <- sample(1:5, size = nrow(crime), replace = T)

cv_fn <- function(alpha_param) {
  model <- cv.glmnet(x = as.matrix(crime[, 1:15]), y = as.matrix(crime[, 16]), 
                 family = 'gaussian', alpha = alpha_param, foldid = folds)
  
  model_info <- data.frame('LL' = log(model$lambda),
                           'mse' = model$cvm,
                           'alpha_val' = alpha_param)
  return(model_info)
}

graph_data <- data.frame()
for (alpha_param in seq(0.1, .95, 0.05)) {
  graph_data <- bind_rows(graph_data, cv_fn(alpha_param))
}
ggplot(data = graph_data) +
  geom_line(aes(x = LL, y = mse, color = factor(alpha_val)))

graph_data[graph_data$mse == min(graph_data$mse), ]

```
Comparing across values of alpha, the best response was an alpha of 0.1. To get a closer look:
```{r}
model.75 <- cv.glmnet(x = as.matrix(crime[, 1:15]), y = as.matrix(crime[, 16]), 
                 family = 'gaussian', alpha = 0.1, foldid = folds)
plot(model.75)
coef(model.75, s = 'lambda.min')

```
Compared with the LASSO model, the Elastic Net includes Pop in its best estimate. This is to be expected - the LASSO model can more easily set variable coefficients to 0.

# Q2

Internet articles are frequently using titles and headlines that are sensationalized or evocative in order to drive traffic. These 'clickbait' articles serve to increase page views and provide ad revenue to their publishers. Since the articles frequently show up in ad spaces from other websites, the publisher might try out multiple versions of the headline shown in the ad in order to see how well each one performs. Since each user click provids the publisher revenue, a milti-armed bandit approach would likely be favorable to a simpler A/B testing routine.

# Q3

```{r}
houses <- FrF2(nruns = 16, nfactors = 10)
houses
```
The matrix above lists the necessary combinations of features to best balance the data collected on individual factors' effects, as well as their effects when combined with other features. Potential buyers' reactions (something like a 0-100 rating of the house or even a simple 'would buy' vs. 'would not buy' response) can then be used to run a regression and determine which features are the biggest drivers of buyer interest.

# Q4

## Binomial

We might consider basketball free throw attempts a binomial data generating process. Though crowd noise, game importance, and stress levels of a given shot due to a close game can fluctuate within and across games, a shooter takes every shot from the same distance and using the same target. Skilled shooters may have p = 0.9, while weak shooters may be lower than 0.4.

## Geometric

The success of a brute-force password cracking machine likely follows a geometric distribution. For example, if a pin number is 3 digits that range from 0-9 each, there are 10,000 possible combinations. The probability of randomly guessing correctly is then p = 0.001. If a cracking tool starts trying random passwords without repeating guesses, then the probability that it takes N failures before success is 0.001 * 0.999^N.

```{r}
data <- data.frame(n = 1:10000)
data %<>%
  mutate(
    p = 0.001 * 0.999**n
  )

ggplot(data = data) +
  geom_line(aes(x = n, y = p))

sum(data$p)
```

## Poisson

When weekly homework assignments are made available, then the submission rate of student answers likely follow a poisson distribution. The average hourly submission rate over a 96 hour period is likely pretty low. There are probably low counts of submissions right after the homework is released, and higher counts towards the deadline. Overall, I would expect a high count of hours with 0 or 1 submissions and far fewer periods with 10+ submissions. This count model would follow a Poisson distribution.

## Exponential

Our daycare opens at 6:30 AM. They must plan ahead to make sure that they are adequately staffed to meet child-caretaker ratio requirements. The inter-arrival time of children should follow an exponential distribution.

## Weibull

I had to replace my last cell phone after the battery eventually stopped holding a charge. The length of time before failure for the battery type used in my phone might follow a Weibull distribution with a k > 1 shape parameter. Like most electronics, batteries degrade over time. The likelihood of a battery losing capacity in the first couple months is low, but as it is cycled over and over, heated and cooled, and exposed to moisture the likelihood that it gives up and breaks increases.